solr、Lucene 是一个高效的，基于Java 的全文检索库。
我们生活中的数据总体分为两种：结构化数据 和非结构化数据 。
结构化数据： 
		指具有固定格式或有限长度的数据，如数据库，元数据等。
非结构化数据（全文数据）： 
		指不定长或无固定格式的数据，如邮件，word文档、XML，HTML等。


对结构化数据的搜索 ：如对数据库的搜索，用SQL语句。再如对元数据的搜索，如利用windows搜索对文件名，类型，修改时间进行搜索等。
对非结构化数据的搜索 ：如利用windows的搜索也可以搜索文件内容，Linux下的grep命令，再如用Google和百度可以搜索大量内容数据。

对非结构化数据也即对全文数据的搜索主要有两种方法：

一、顺序扫描法 (Serial Scanning)：
所谓顺序扫描，比如要找内容包含某一个字符串的文件，就是一个文档一个文档的看，对于每一个文档，从头看到尾，
如果此文档包含此字符串，则此文档为我们要找的文件，接着看下一个文件，直到扫描完所有的文件。

二、索引法（反向索引/全文检索）：
将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。
这部分从非结构化数据中提取出的然后重新组织的信息，我们称之索引 。
这种先建立索引，再对索引进行搜索的过程就叫全文检索(Full-text Search) 。

全文检索大体分两个过程，索引创建 (Indexing) 和搜索索引 (Search) 。
索引创建：
将现实世界中所有的结构化和非结构化数据提取信息，创建索引的过程。
搜索索引：
就是得到用户的查询请求，搜索创建的索引，然后返回结果的过程。

非结构化数据中所存储的信息是每个文件包含哪些字符串，也即已知文件，欲求字符串相对容易，也即是从文件到字符串的映射。
而我们想搜索的信息是哪些文件包含此字符串，也即已知字符串，欲求文件，也即从字符串到文件的映射。两者恰恰相反。
于是如果索引总能够保存从字符串到文件的映射，则会大大提高搜索速度。

由于从字符串到文件的映射是文件到字符串映射的反向过程，于是保存这种信息的索引称为反向索引 。


全文检索的确加快了搜索的速度，但是多了索引的过程，两者加起来不一定比顺序扫描快多少。
的确，加上索引的过程，全文检索不一定比顺序扫描快，尤其是在数据量小的时候更是如此。
而对一个很大量的数据创建索引也是一个很慢的过程。
然而两者还是有区别的，顺序扫描是每次都要扫描，而创建索引的过程仅仅需要一次，以后便是一劳永逸的了，
每次搜索，创建索引的过程不必经过，仅仅搜索创建好的索引就可以了。
这也是全文搜索相对于顺序扫描的优势之一：一次索引，多次使用。



配置schema.xml文件 定义field，设置indexed属性，若为true则是索引，否则不是索引。
type主要有：int、long、string、textMaxWord、textSimple、text_general。另外还有一些required、sorted属性。
通过copyField指定多个属性到索引字段上。例如：
<copyField source="title"     dest="text"/>
<copyField source="content"   dest="text"/>

对应的java文件，通过@Field(name)指定对应的字段。


具体流程
1、对文档进行处理分词，例如将“我是中国人”，分为“我是“、”中国“、”国人“称为term。其中包括去标点符号、变小写等操作。
接着建索引词组，建立term到文档的指向关系，合并相同的词(Term)成为文档倒排(Posting List)链表。
2、搜索索引
首先对查询语句进行分析处理成一棵语法树，有关键字和普通词语组成。
接着，进行权重计算。权重表示该词在文档中的重要程度，越重要的词当然权重越高，因此在计算文档相关性时影响力就更大。
通过词之间的权重得到文档相关性的过程叫做空间向量模型算法(Vector Space Model)
影响一个词在文档中的重要性主要有两个方面：
Term Frequencey（tf），Term在此文档中出现的频率，ft越大表示越重要
Document Frequency（df），表示有多少文档中出现过这个Trem，df越大表示越不重要
物以希为贵，大家都有的东西，自然就不那么贵重了，只有你专有的东西表示这个东西很珍贵，

分词器：
StandardAnalyzer//单字分词器
IKAnalyzer/二分法分词
CJKAnalyzer/庖丁分词器， 中文分词

对index 和 query 都设置analyzer
公司使用 Tokenizer 和 Filter 结合一起
tokenizer：接收text（通过从solr那里获得一个reader来读取文本），拆分成tokens，输出token stream；将句子拆成一个个的词
filter：接收token stream，对每个token进行处理（比如：替换、丢弃、不理），输出token stream；经过Tokenizer分词之后，再进行的继续处理，比如全转成小写，时态处理， 去掉语气词等。。。
例如：
<fieldType name="text_general" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <!-- in this example, we will only use synonyms at query time
        <filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/>
        -->
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>
	

具体使用时：
SolrQuery query 对象

query.addFilterQuery(new String[]{"content:(001 or 002)"});
query.setQuery(String.format("\"%s\"", new Object[] { keyword }));
query.setParam("q", new String[] { keyword });
query.setParam("df", new String[] { "id" });

query.setSort("content",SolrQuery.ORDER.asc);
query.setStart(10);
query.setRows(100);

QueryResponse rsp = SolrServer.query(params);
List<NewsIndexData> newsIndexData = rsp.getBeans(NewsIndexData.class);



